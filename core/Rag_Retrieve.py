import os
import json
import pickle
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
import jieba
from rank_bm25 import BM25Okapi
from dataclasses import dataclass
from openai import OpenAI
import hashlib


@dataclass
class DocumentChunk:
    """æ–‡æ¡£ç‰‡æ®µæ•°æ®ç±»"""
    id: str
    content: str
    source_file: str
    chunk_index: int
    start_pos: int
    end_pos: int
    metadata: Dict[str, Any] = None


class ChineseTextSplitter:
    """ä¸­æ–‡æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str, source_file: str = "") -> List[DocumentChunk]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆchunks

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            source_file: æºæ–‡ä»¶å

        Returns:
            List[DocumentChunk]: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not text.strip():
            return []

        chunks = []
        text_length = len(text)

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]

        current_chunk = ""
        current_start = 0
        chunk_index = 0

        for paragraph in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡chunk_sizeï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if len(paragraph) > self.chunk_size:
                if current_chunk:
                    chunk_id = self._generate_chunk_id(source_file, chunk_index)
                    chunks.append(DocumentChunk(
                        id=chunk_id,
                        content=current_chunk.strip(),
                        source_file=source_file,
                        chunk_index=chunk_index,
                        start_pos=current_start,
                        end_pos=current_start + len(current_chunk)
                    ))
                    chunk_index += 1
                    current_chunk = ""

                # åˆ†å‰²é•¿æ®µè½
                sub_chunks = self._split_long_paragraph(paragraph)
                for sub_chunk in sub_chunks:
                    chunk_id = self._generate_chunk_id(source_file, chunk_index)
                    chunks.append(DocumentChunk(
                        id=chunk_id,
                        content=sub_chunk,
                        source_file=source_file,
                        chunk_index=chunk_index,
                        start_pos=text.find(sub_chunk),
                        end_pos=text.find(sub_chunk) + len(sub_chunk)
                    ))
                    chunk_index += 1
            else:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°chunk
                if len(current_chunk) + len(paragraph) > self.chunk_size and current_chunk:
                    # ä¿å­˜å½“å‰chunk
                    chunk_id = self._generate_chunk_id(source_file, chunk_index)
                    chunks.append(DocumentChunk(
                        id=chunk_id,
                        content=current_chunk.strip(),
                        source_file=source_file,
                        chunk_index=chunk_index,
                        start_pos=current_start,
                        end_pos=current_start + len(current_chunk)
                    ))
                    chunk_index += 1

                    # å¤„ç†é‡å 
                    if self.chunk_overlap > 0:
                        overlap_text = current_chunk[-self.chunk_overlap:]
                        current_chunk = overlap_text + "\n" + paragraph
                    else:
                        current_chunk = paragraph
                    current_start = text.find(paragraph) - len(overlap_text) if self.chunk_overlap > 0 else text.find(
                        paragraph)
                else:
                    # æ·»åŠ åˆ°å½“å‰chunk
                    if current_chunk:
                        current_chunk += "\n" + paragraph
                    else:
                        current_chunk = paragraph
                        current_start = text.find(paragraph)

        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk.strip():
            chunk_id = self._generate_chunk_id(source_file, chunk_index)
            chunks.append(DocumentChunk(
                id=chunk_id,
                content=current_chunk.strip(),
                source_file=source_file,
                chunk_index=chunk_index,
                start_pos=current_start,
                end_pos=current_start + len(current_chunk)
            ))

        return chunks

    def _split_long_paragraph(self, paragraph: str) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½"""
        chunks = []
        sentences = [s.strip() for s in paragraph.split('ã€‚') if s.strip()]

        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:
                chunks.append(current_chunk + "ã€‚")
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += "ã€‚" + sentence
                else:
                    current_chunk = sentence

        if current_chunk:
            chunks.append(current_chunk + "ã€‚" if not current_chunk.endswith('ã€‚') else current_chunk)

        return chunks

    def _generate_chunk_id(self, source_file: str, chunk_index: int) -> str:
        """ç”Ÿæˆchunk ID"""
        return f"{source_file}_{chunk_index}_{hashlib.md5(f'{source_file}_{chunk_index}'.encode()).hexdigest()[:8]}"


class DocumentVectorStore:
    """æ–‡æ¡£å‘é‡å­˜å‚¨"""

    def __init__(self, api_key: str = None, model_name: str = "text-embedding-3-small", use_embeddings: bool = True):
        self.api_key = api_key
        self.model_name = model_name
        self.use_embeddings = use_embeddings

        # æ ¹æ®APIå¯†é’¥åˆ¤æ–­ä½¿ç”¨å“ªä¸ªæœåŠ¡
        if api_key and use_embeddings:
            if api_key.startswith("sk-") and "deepseek" not in api_key:
                # OpenAI API
                self.client = OpenAI(api_key=api_key)
                self.api_type = "openai"
            else:
                # DeepSeek API
                self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
                self.api_type = "deepseek"
                # DeepSeekæš‚ä¸æ”¯æŒembeddingï¼Œæ”¹ç”¨çº¯BM25
                self.use_embeddings = False
                print("âš ï¸ DeepSeek APIæš‚ä¸æ”¯æŒembeddingï¼Œå°†ä½¿ç”¨çº¯BM25æ£€ç´¢")
        else:
            self.client = None
            self.api_type = None
            self.use_embeddings = False
            print("â„¹ï¸ æœªå¯ç”¨å‘é‡æ£€ç´¢ï¼Œå°†ä½¿ç”¨çº¯BM25æ£€ç´¢")

        # å­˜å‚¨chunkså’Œå‘é‡
        self.chunks: List[DocumentChunk] = []
        self.embeddings: List[List[float]] = []
        self.chunk_index: Dict[str, int] = {}  # chunk_id -> index

        # BM25ç´¢å¼•
        self.bm25_index = None
        self.tokenized_chunks = []

    def add_documents(self, chunks: List[DocumentChunk]):
        """æ·»åŠ æ–‡æ¡£ç‰‡æ®µ"""
        print(f"æ­£åœ¨å¤„ç† {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")

        # ç”Ÿæˆembeddingsï¼ˆå¦‚æœå¯ç”¨ï¼‰
        new_embeddings = []

        if self.use_embeddings and self.client:
            batch_size = 50

            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                batch_texts = [chunk.content for chunk in batch_chunks]

                print(f"æ­£åœ¨ç”Ÿæˆembeddings: {i + 1}-{min(i + batch_size, len(chunks))} / {len(chunks)}")

                try:
                    response = self.client.embeddings.create(
                        model=self.model_name,
                        input=batch_texts
                    )
                    batch_embeddings = [item.embedding for item in response.data]
                    new_embeddings.extend(batch_embeddings)
                except Exception as e:
                    print(f"ç”Ÿæˆembeddingså¤±è´¥: {e}")
                    print("âš ï¸ å°†ç¦ç”¨å‘é‡æ£€ç´¢ï¼Œä½¿ç”¨çº¯BM25æ£€ç´¢")
                    self.use_embeddings = False
                    new_embeddings = []
                    break
        else:
            print("â„¹ï¸ è·³è¿‡å‘é‡embeddingç”Ÿæˆï¼Œä½¿ç”¨BM25æ£€ç´¢")

        # æ›´æ–°å­˜å‚¨
        start_idx = len(self.chunks)
        for i, chunk in enumerate(chunks):
            self.chunk_index[chunk.id] = start_idx + i

        self.chunks.extend(chunks)
        self.embeddings.extend(new_embeddings)

        # æ„å»ºBM25ç´¢å¼•
        self._build_bm25_index()

        print(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•"""
        print("æ­£åœ¨æ„å»ºBM25ç´¢å¼•...")

        # åˆ†è¯
        self.tokenized_chunks = []
        for chunk in self.chunks:
            tokens = self._tokenize(chunk.content)
            self.tokenized_chunks.append(tokens)

        # æ„å»ºBM25
        if self.tokenized_chunks:
            self.bm25_index = BM25Okapi(self.tokenized_chunks, k1=1.5, b=0.75)

        print("âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ")

    def _tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        # åœç”¨è¯
        stop_words = {
            'æ˜¯', 'ä¸€ç§', 'çš„', 'äº†', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å› ä¸º', 'æ‰€ä»¥',
            'è¿™', 'é‚£', 'äº›', 'ä¸ª', 'æŠŠ', 'è¢«', 'è®©', 'ä½¿', 'å¾—', 'ç€', 'è¿‡', 'æ¥', 'å»',
            'ä¸Š', 'ä¸‹', 'ä¸­', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³', 'æœ‰', 'ä¼š', 'èƒ½', 'å¯ä»¥',
            'å°±', 'éƒ½', 'è¦', 'è¯´', 'å¯¹', 'ä¸º', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'æ—¶å€™', 'åº”è¯¥'
        }

        tokens = list(jieba.cut(text))
        filtered_tokens = []

        for token in tokens:
            token = token.strip()
            if len(token) > 1 and token not in stop_words:
                filtered_tokens.append(token)

        return filtered_tokens

    def similarity_search(self, query: str, k: int = 5, search_type: str = "hybrid") -> List[
        Tuple[DocumentChunk, float]]:
        """
        ç›¸ä¼¼æ€§æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ ("vector", "bm25", "hybrid")

        Returns:
            List[Tuple[DocumentChunk, float]]: (æ–‡æ¡£ç‰‡æ®µ, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not self.chunks:
            return []

        # å¦‚æœä¸æ”¯æŒå‘é‡æœç´¢ï¼Œå¼ºåˆ¶ä½¿ç”¨BM25
        if not self.use_embeddings and search_type in ["vector", "hybrid"]:
            print(f"âš ï¸ å‘é‡æœç´¢ä¸å¯ç”¨ï¼Œä½¿ç”¨BM25æœç´¢")
            search_type = "bm25"

        if search_type == "vector":
            return self._vector_search(query, k)
        elif search_type == "bm25":
            return self._bm25_search(query, k)
        else:  # hybrid
            return self._hybrid_search(query, k)

    def _vector_search(self, query: str, k: int) -> List[Tuple[DocumentChunk, float]]:
        """å‘é‡ç›¸ä¼¼æ€§æœç´¢"""
        if not self.use_embeddings or not self.client:
            print("âš ï¸ å‘é‡æœç´¢ä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨BM25æœç´¢")
            return []

        try:
            # è·å–æŸ¥è¯¢å‘é‡
            response = self.client.embeddings.create(
                model=self.model_name,
                input=[query]
            )
            query_embedding = response.data[0].embedding

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_np = np.array(query_embedding)
            embeddings_np = np.array(self.embeddings)

            # è®¡ç®—ç‚¹ç§¯
            similarities = np.dot(embeddings_np, query_np)
            # å½’ä¸€åŒ–
            norms = np.linalg.norm(embeddings_np, axis=1) * np.linalg.norm(query_np)
            similarities = similarities / norms

            # è·å–top-kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:k]

            results = []
            for idx in top_indices:
                chunk = self.chunks[idx]
                score = similarities[idx]
                results.append((chunk, float(score)))

            return results

        except Exception as e:
            print(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            print("âš ï¸ å›é€€åˆ°BM25æœç´¢")
            return self._bm25_search(query, k)

    def _bm25_search(self, query: str, k: int) -> List[Tuple[DocumentChunk, float]]:
        """BM25æœç´¢"""
        if not self.bm25_index:
            return []

        query_tokens = self._tokenize(query)
        scores = self.bm25_index.get_scores(query_tokens)

        # è·å–top-kç»“æœ
        top_indices = np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            if scores[idx] > 0:  # åªè¿”å›æœ‰å¾—åˆ†çš„ç»“æœ
                chunk = self.chunks[idx]
                score = scores[idx]
                results.append((chunk, float(score)))

        return results

    def _hybrid_search(self, query: str, k: int) -> List[Tuple[DocumentChunk, float]]:
        """æ··åˆæœç´¢ï¼ˆå‘é‡ + BM25ï¼‰"""
        # å¦‚æœå‘é‡æœç´¢ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨BM25
        if not self.use_embeddings:
            return self._bm25_search(query, k)

        # è·å–ä¸¤ç§æœç´¢çš„ç»“æœ
        vector_results = self._vector_search(query, k * 2)
        bm25_results = self._bm25_search(query, k * 2)

        # å¦‚æœå‘é‡æœç´¢å¤±è´¥ï¼Œåªä½¿ç”¨BM25ç»“æœ
        if not vector_results:
            return bm25_results

        # å½’ä¸€åŒ–åˆ†æ•°å¹¶åˆå¹¶
        chunk_scores = {}

        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        vector_scores = [score for _, score in vector_results]
        if vector_scores:
            max_vector_score = max(vector_scores)
            min_vector_score = min(vector_scores)
            score_range = max_vector_score - min_vector_score

            for chunk, score in vector_results:
                # å½’ä¸€åŒ–åˆ°0-1
                normalized_score = (score - min_vector_score) / score_range if score_range > 0 else 0
                chunk_scores[chunk.id] = chunk_scores.get(chunk.id, 0) + normalized_score * 0.6  # å‘é‡æƒé‡60%

        # å¤„ç†BM25æœç´¢ç»“æœ
        bm25_scores = [score for _, score in bm25_results]
        if bm25_scores:
            max_bm25_score = max(bm25_scores)
            min_bm25_score = min(bm25_scores)
            score_range = max_bm25_score - min_bm25_score

            for chunk, score in bm25_results:
                # å½’ä¸€åŒ–åˆ°0-1
                normalized_score = (score - min_bm25_score) / score_range if score_range > 0 else 0
                chunk_scores[chunk.id] = chunk_scores.get(chunk.id, 0) + normalized_score * 0.4  # BM25æƒé‡40%

        # è·å–chunksæ˜ å°„
        chunk_map = {chunk.id: chunk for chunk in self.chunks}

        # æ’åºå¹¶è¿”å›top-k
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)[:k]

        results = []
        for chunk_id, score in sorted_chunks:
            if chunk_id in chunk_map:
                results.append((chunk_map[chunk_id], score))

        return results

    def save_index(self, filepath: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        data = {
            'chunks': self.chunks,
            'embeddings': self.embeddings,
            'chunk_index': self.chunk_index,
            'tokenized_chunks': self.tokenized_chunks
        }

        with open(filepath, 'wb') as f:
            pickle.dump(data, f)

        print(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {filepath}")

    def load_index(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        if not os.path.exists(filepath):
            print(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False

        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)

            self.chunks = data['chunks']
            self.embeddings = data['embeddings']
            self.chunk_index = data['chunk_index']
            self.tokenized_chunks = data.get('tokenized_chunks', [])

            # é‡å»ºBM25ç´¢å¼•
            if self.tokenized_chunks:
                self.bm25_index = BM25Okapi(self.tokenized_chunks, k1=1.5, b=0.75)

            print(f"âœ… ç´¢å¼•å·²ä» {filepath} åŠ è½½ï¼ŒåŒ…å« {len(self.chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            return True

        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self, api_key: str = None, knowledge_base_path: str = "", index_cache_path: str = None,
                 use_embeddings: bool = True):
        self.api_key = api_key
        self.knowledge_base_path = knowledge_base_path
        self.index_cache_path = index_cache_path or "rag_index.pkl"

        # åˆå§‹åŒ–ç»„ä»¶
        self.text_splitter = ChineseTextSplitter(chunk_size=500, chunk_overlap=50)
        self.vector_store = DocumentVectorStore(api_key, use_embeddings=use_embeddings)

        # å¦‚æœæœ‰APIå¯†é’¥ï¼Œåˆå§‹åŒ–å®¢æˆ·ç«¯
        if api_key:
            if api_key.startswith("sk-") and "deepseek" not in api_key:
                self.client = OpenAI(api_key=api_key)
            else:
                self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        else:
            self.client = None

        # æ–‡ä»¶æ‰©å±•å
        self.supported_extensions = ['.md', '.txt']

    def build_index(self, force_rebuild: bool = False):
        """æ„å»ºæ–‡æ¡£ç´¢å¼•"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
        if not force_rebuild and os.path.exists(self.index_cache_path):
            if self.vector_store.load_index(self.index_cache_path):
                return

        print(f"å¼€å§‹æ„å»ºæ–‡æ¡£ç´¢å¼•...")
        print(f"æ‰«æè·¯å¾„: {self.knowledge_base_path}")

        if not os.path.exists(self.knowledge_base_path):
            print(f"é”™è¯¯: çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {self.knowledge_base_path}")
            return

        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        all_chunks = []
        file_count = 0

        for root, dirs, files in os.walk(self.knowledge_base_path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in self.supported_extensions):
                    file_path = os.path.join(root, file)
                    print(f"å¤„ç†æ–‡ä»¶: {file}")

                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        if content.strip():
                            chunks = self.text_splitter.split_text(content, file)
                            all_chunks.extend(chunks)
                            file_count += 1
                            print(f"  âœ… ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")

                    except Exception as e:
                        print(f"  âŒ å¤„ç†å¤±è´¥: {e}")

        print(f"\næ€»è®¡å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

        if all_chunks:
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            self.vector_store.add_documents(all_chunks)

            # ä¿å­˜ç´¢å¼•
            self.vector_store.save_index(self.index_cache_path)
            print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹")

    def retrieve_documents(self, query: str, top_k: int = 5, search_type: str = "hybrid") -> List[
        Tuple[DocumentChunk, float]]:
        """
        æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚æ‘˜è¦ï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ ("vector", "bm25", "hybrid")

        Returns:
            List[Tuple[DocumentChunk, float]]: ç›¸å…³æ–‡æ¡£ç‰‡æ®µå’Œåˆ†æ•°
        """
        print(f"\nğŸ” æ£€ç´¢æŸ¥è¯¢: {query[:100]}{'...' if len(query) > 100 else ''}")
        print(f"æœç´¢ç±»å‹: {search_type}, è¿”å›æ•°é‡: {top_k}")

        results = self.vector_store.similarity_search(query, top_k, search_type)

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ:")
        for i, (chunk, score) in enumerate(results, 1):
            print(f"{i}. [{chunk.source_file}] å¾—åˆ†: {score:.4f}")
            print(f"   å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")
            print()

        return results

    def retrieve_by_files(self, query: str, target_files: List[str], top_k: int = 5, search_type: str = "hybrid") -> \
    List[Tuple[DocumentChunk, float]]:
        """
        åœ¨æŒ‡å®šæ–‡ä»¶ä¸­æ£€ç´¢ç›¸å…³å†…å®¹

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            target_files: ç›®æ ‡æ–‡ä»¶åˆ—è¡¨
            top_k: æ¯ä¸ªæ–‡ä»¶è¿”å›çš„ç‰‡æ®µæ•°é‡
            search_type: æœç´¢ç±»å‹

        Returns:
            List[Tuple[DocumentChunk, float]]: ç›¸å…³æ–‡æ¡£ç‰‡æ®µå’Œåˆ†æ•°
        """
        print(f"\nğŸ¯ åœ¨æŒ‡å®šæ–‡ä»¶ä¸­æ£€ç´¢:")
        for file in target_files:
            print(f"  - {file}")

        # è·å–æ‰€æœ‰ç»“æœ
        all_results = self.vector_store.similarity_search(query, len(self.vector_store.chunks), search_type)

        # æŒ‰æ–‡ä»¶è¿‡æ»¤
        file_results = []
        file_counts = {}

        for chunk, score in all_results:
            if chunk.source_file in target_files:
                if file_counts.get(chunk.source_file, 0) < top_k:
                    file_results.append((chunk, score))
                    file_counts[chunk.source_file] = file_counts.get(chunk.source_file, 0) + 1

        print(f"âœ… åœ¨ç›®æ ‡æ–‡ä»¶ä¸­æ‰¾åˆ° {len(file_results)} ä¸ªç›¸å…³ç‰‡æ®µ")
        return file_results

    def generate_context(self, query: str, retrieved_chunks: List[Tuple[DocumentChunk, float]]) -> str:
        """
        æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µç”Ÿæˆä¸Šä¸‹æ–‡

        Args:
            query: åŸå§‹æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ

        Returns:
            str: ç”Ÿæˆçš„ä¸Šä¸‹æ–‡
        """
        if not retrieved_chunks:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"

        # æŒ‰æ–‡ä»¶åˆ†ç»„
        file_groups = {}
        for chunk, score in retrieved_chunks:
            if chunk.source_file not in file_groups:
                file_groups[chunk.source_file] = []
            file_groups[chunk.source_file].append((chunk, score))

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        context_parts.append(
            f"åŸºäºæŸ¥è¯¢ã€Œ{query[:100]}{'...' if len(query) > 100 else ''}ã€ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n")

        for file_name, chunks in file_groups.items():
            context_parts.append(f"## æ¥æºæ–‡ä»¶: {file_name}\n")

            for i, (chunk, score) in enumerate(chunks, 1):
                context_parts.append(f"### ç‰‡æ®µ {i} (ç›¸å…³åº¦: {score:.3f})")
                context_parts.append(f"{chunk.content}\n")

        return "\n".join(context_parts)

    def get_file_statistics(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.vector_store.chunks:
            return {"æ€»æ–‡æ¡£ç‰‡æ®µæ•°": 0, "æ–‡ä»¶æ•°": 0, "å¹³å‡ç‰‡æ®µé•¿åº¦": 0}

        file_counts = {}
        total_length = 0

        for chunk in self.vector_store.chunks:
            file_counts[chunk.source_file] = file_counts.get(chunk.source_file, 0) + 1
            total_length += len(chunk.content)

        return {
            "æ€»æ–‡æ¡£ç‰‡æ®µæ•°": len(self.vector_store.chunks),
            "æ–‡ä»¶æ•°": len(file_counts),
            "å¹³å‡ç‰‡æ®µé•¿åº¦": total_length // len(self.vector_store.chunks),
            "æ–‡ä»¶åˆ†å¸ƒ": file_counts
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    API_KEY = "sk-2f4ea37debc1420d82379d20963cba30"  # DeepSeek APIå¯†é’¥
    KNOWLEDGE_BASE_PATH = "..\config\strategies"

    # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆé’ˆå¯¹DeepSeek APIä½¿ç”¨çº¯BM25ï¼‰
    rag = RAGSystem(
        api_key=API_KEY,
        knowledge_base_path=KNOWLEDGE_BASE_PATH,
        index_cache_path="rag_index.pkl",
        use_embeddings=False  # å¯¹äºDeepSeek APIï¼Œè®¾ä¸ºFalseä½¿ç”¨çº¯BM25
    )

    # æ„å»ºç´¢å¼•ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰
    print("ğŸš€ æ„å»ºæ–‡æ¡£ç´¢å¼•...")
    rag.build_index(force_rebuild=False)  # è®¾ä¸ºTrueå¼ºåˆ¶é‡å»º

    # ç¤ºä¾‹æŸ¥è¯¢ï¼ˆæ‘˜è¦ï¼‰
    abstract_query = """
    æ–°å“æ¨å¹¿å¦‚ä½•é€šè¿‡äºšé©¬é€Šå¹¿å‘Šå®ç°çˆ†å•çš„æ ¸å¿ƒç­–ç•¥åŒ…æ‹¬å››å±‚æµé‡æœºåˆ¶ä¸æ™ºèƒ½ç®—æ³•ä¼˜åŒ–ã€‚
    ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥é€‰è¯æŠ•æ”¾ï¼Œå­˜åœ¨ACOSåé«˜ã€æµé‡è·å–å›°éš¾ç­‰é—®é¢˜ã€‚
    DeepBIé€šè¿‡è‡ªåŠ¨åŠ è¯ç­–ç•¥ã€è‡ªåŠ¨åŠ ASINç­–ç•¥ä»¥åŠé¢„ç®—æ™ºèƒ½åˆ†é…å®ç°ç²¾å‡†æµé‡è·å–ã€‚
    """

    # å…¨å±€æ£€ç´¢ï¼ˆä½¿ç”¨BM25ï¼‰
    print("\n" + "=" * 60)
    print("1. å…¨å±€æ–‡æ¡£æ£€ç´¢ï¼ˆBM25ï¼‰")
    print("=" * 60)

    results = rag.retrieve_documents(
        query=abstract_query,
        top_k=8,
        search_type="bm25"  # ä½¿ç”¨BM25æœç´¢
    )

    # ç”Ÿæˆæ£€ç´¢ä¸Šä¸‹æ–‡
    if results:
        context = rag.generate_context(abstract_query, results)
        print("\nğŸ“„ ç”Ÿæˆçš„ä¸Šä¸‹æ–‡:")
        print("-" * 40)
        print(context[:1000] + "..." if len(context) > 1000 else context)

    # æŒ‡å®šæ–‡ä»¶æ£€ç´¢
    print("\n" + "=" * 60)
    print("2. æŒ‡å®šæ–‡ä»¶æ£€ç´¢")
    print("=" * 60)

    target_files = ["è‡ªåŠ¨åŠ è¯ç­–ç•¥.md", "è‡ªåŠ¨åŠ ASINç­–ç•¥.txt", "å››å±‚æµé‡æœºåˆ¶.md"]
    file_results = rag.retrieve_by_files(
        query=abstract_query,
        target_files=target_files,
        top_k=3,
        search_type="bm25"
    )

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = rag.get_file_statistics()
    print(f"\nğŸ“Š æ–‡æ¡£åº“ç»Ÿè®¡:")
    print("-" * 40)
    for key, value in stats.items():
        if key != "æ–‡ä»¶åˆ†å¸ƒ":
            print(f"{key}: {value}")

    if "æ–‡ä»¶åˆ†å¸ƒ" in stats and stats["æ–‡ä»¶åˆ†å¸ƒ"]:
        print(f"\nğŸ“ æ–‡ä»¶åˆ†å¸ƒ:")
        for file, count in stats["æ–‡ä»¶åˆ†å¸ƒ"].items():
            print(f"  {file}: {count} ä¸ªç‰‡æ®µ")

    # æ¼”ç¤ºä¸åŒæœç´¢ç±»å‹çš„æ•ˆæœ
    print("\n" + "=" * 60)
    print("3. æœç´¢ç±»å‹å¯¹æ¯”")
    print("=" * 60)

    test_query = "è‡ªåŠ¨åŠ è¯ç­–ç•¥å¦‚ä½•ä¼˜åŒ–ACOS"

    bm25_results = rag.retrieve_documents(test_query, top_k=3, search_type="bm25")
    print(f"\nğŸ” BM25æœç´¢ç»“æœ ({len(bm25_results)} ä¸ª):")
    for i, (chunk, score) in enumerate(bm25_results, 1):
        print(f"{i}. [{chunk.source_file}] å¾—åˆ†: {score:.4f}")
        print(f"   å†…å®¹: {chunk.content[:100]}...")

    # å¦‚æœå¯ç”¨äº†å‘é‡æœç´¢ï¼Œä¹Ÿæ˜¾ç¤ºå¯¹æ¯”
    if rag.vector_store.use_embeddings:
        try:
            vector_results = rag.retrieve_documents(test_query, top_k=3, search_type="vector")
            print(f"\nğŸ§  å‘é‡æœç´¢ç»“æœ ({len(vector_results)} ä¸ª):")
            for i, (chunk, score) in enumerate(vector_results, 1):
                print(f"{i}. [{chunk.source_file}] å¾—åˆ†: {score:.4f}")
                print(f"   å†…å®¹: {chunk.content[:100]}...")
        except:
            print("\nâš ï¸ å‘é‡æœç´¢ä¸å¯ç”¨")