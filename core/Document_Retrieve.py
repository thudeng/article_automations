from rank_bm25 import BM25Okapi
import jieba
import numpy as np
import os
import glob
from typing import List, Tuple, Optional, Dict


class DocumentRetriever:
    """æ™ºèƒ½æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ"""

    def __init__(self, knowledge_base_path: str, file_extensions: List[str] = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£æ£€ç´¢å™¨

        Args:
            knowledge_base_path: çŸ¥è¯†åº“æ–‡ä»¶å¤¹è·¯å¾„
            file_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åï¼Œé»˜è®¤ä¸º ['.md', '.txt']
        """
        self.knowledge_base_path = knowledge_base_path
        self.file_extensions = file_extensions or ['.md', '.txt']

        # å­˜å‚¨æ–‡æ¡£æ•°æ®
        self.documents = []
        self.file_names = []
        self.tokenized_documents = []
        self.tokenized_titles = []

        # BM25 æ¨¡å‹
        self.bm25_content = None
        self.bm25_title = None

        # è¯­ä¹‰å…³ç³»åº“
        self.semantic_relations = self._init_semantic_relations()

        # åœç”¨è¯
        self.stop_words = {
            'æ˜¯', 'ä¸€ç§', 'çš„', 'äº†', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å› ä¸º', 'æ‰€ä»¥',
            'è¿™', 'é‚£', 'äº›', 'ä¸ª', 'æŠŠ', 'è¢«', 'è®©', 'ä½¿', 'å¾—', 'ç€', 'è¿‡', 'æ¥', 'å»',
            'ä¸Š', 'ä¸‹', 'ä¸­', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³', 'æœ‰', 'ä¼š', 'èƒ½', 'å¯ä»¥',
            'å°±', 'éƒ½', 'è¦', 'è¯´', 'å¯¹', 'ä¸º', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'æ—¶å€™', 'åº”è¯¥',
            'æˆ–è€…', 'ROI', 'ACOS', 'äºšé©¬é€Š', 'å¹¿å‘Š', 'ä¸ºä»€ä¹ˆ', 'è¿è¥', 'å¾€å¾€',"é¦–å…ˆ","ç„¶å","æœ€å","æ€»ç»“","è¿›é˜¶","æŒ‡å—"
        }

        self.unmeaningful_chars = {
            "ä¸ºä»€ä¹ˆ", "å‘¢", "çš„", "åœ°", "å¾—", "å¦‚ä½•", "å®ç°", "è¿˜æ˜¯",
            'ä»€ä¹ˆ', 'æ—¶å€™', 'æ€ä¹ˆ', 'ï¼Ÿ', 'å‘¢'
        }

        # åŠ è½½æ–‡æ¡£
        self._load_documents()

    def _init_semantic_relations(self) -> Dict[Tuple[str, str], float]:
        """åˆå§‹åŒ–è¯­ä¹‰å…³ç³»åº“"""
        semantic_relations = {
            # æ›å…‰ç›¸å…³
            ('æé«˜', 'æ›å…‰'): 2.0,
            ('å¹¿å‘Š', 'ä¼˜åŒ–'): 1.8,
            ('æ›å…‰', 'ä¼˜åŒ–'): 1.7,
            ('æ›å…‰', 'é¢„ç®—'): 1.5,
            ('å…³é”®è¯', 'åŒ¹é…'): 1.5,
            ('æ›å…‰', 'ç­–ç•¥'): 1.6,
            ('ç«ä»·', 'ä¼˜åŒ–'): 1.4,
            ('å¹¿å‘Š', 'æ•°æ®åˆ†æ'): 1.3,
            ('ç›®æ ‡', 'å—ä¼—'): 1.5,
            ('å¹¿å‘Š', 'æŠ•æ”¾'): 1.2,
            ('æ›å…‰', 'ç‚¹å‡»'): 1.2,
            ('æµé‡', 'æå‡'): 1.5,
            ('æ™ºèƒ½', 'æŠ•æ”¾'): 1.6,
            ('äº§å“', 'æ¨å¹¿'): 1.4,
            ('å¹¿å‘Š', 'æˆæ•ˆ'): 1.3,
            ('æ›å…‰', 'åˆ†æ'): 1.3,
            ('å¹¿å‘Š', 'è‡ªåŠ¨åŒ–'): 1.2,
            ('æŠ•æ”¾', 'ä¼˜åŒ–'): 1.5,
            ('æ›å…‰', 'æå‡'): 2.0,
            ('å¹¿å‘Š', 'ç›‘æ§'): 1.4,
            ('è¥é”€', 'ç­–ç•¥'): 1.5,

            # AIç›¸å…³
            ('AI', 'å¹¿å‘Š'): 1.5,
            ('AI', 'ç­–ç•¥'): 1.7,
            ('AI', 'æŠ•æ”¾'): 1.6,
            ('AI', 'æµé‡'): 1.5,
            ('AI', 'è‡ªåŠ¨'): 1.6,
            ('AI', 'ç®—æ³•'): 1.8,
            ('AI', 'é¢„ç®—'): 1.5,
            ('AI', 'äººå·¥'): 1.6,
            ('AI', 'å®æ—¶'): 1.4,
            ('AI', 'è‡ªåŠ¨åŒ–'): 1.7,
            ('AI', 'ç‰¹ç‚¹'): 1.5,
            ('AI', 'ä¼˜åŠ¿'): 1.8,
            ('AI', 'ä¼˜ç‚¹'): 1.6,
            ('äººå·¥', 'ç¼ºç‚¹'): 1.7,
            ('äººå·¥', 'ä¼ ç»Ÿ'): 1.7,
            ('AI', 'DeepBI'): 1.0,
        }
        return semantic_relations

    def _load_documents(self):
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£"""
        if not os.path.exists(self.knowledge_base_path):
            raise FileNotFoundError(f"çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.knowledge_base_path}")

        # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£è·¯å¾„
        document_paths = []
        for ext in self.file_extensions:
            pattern = os.path.join(self.knowledge_base_path, f"*{ext}")
            document_paths.extend(glob.glob(pattern))

        if not document_paths:
            extensions_str = ', '.join(self.file_extensions)
            raise FileNotFoundError(f"åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extensions_str}")

        print(f"æ‰¾åˆ° {len(document_paths)} ä¸ªæ–‡ä»¶:")

        # è¯»å–æ–‡æ¡£å†…å®¹
        for document_path in document_paths:
            try:
                with open(document_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                    self.documents.append(content)

                    file_name = os.path.basename(document_path)
                    self.file_names.append(file_name)
                    print(f"âœ“ å·²åŠ è½½: {file_name}")
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶å‡ºé”™ {os.path.basename(document_path)}: {e}")
                self.documents.append("")
                self.file_names.append(os.path.basename(document_path))

        # åˆ†è¯å¤„ç†
        self.tokenized_documents = [self._tokenize(doc) for doc in self.documents]
        self.tokenized_titles = [self._tokenize(name) for name in self.file_names]

        # åˆå§‹åŒ–BM25æ¨¡å‹
        self.bm25_content = BM25Okapi(self.tokenized_documents, k1=1.5, b=0.75)
        self.bm25_title = BM25Okapi(self.tokenized_titles, k1=1.5, b=0.75)

        print(f"æ–‡æ¡£åº“åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£")

    def _tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯"""
        tokens = list(jieba.cut(text))
        meaningful_chars = {'å¹¿å‘Š', 'æŠ•æ”¾', 'æ›å…‰', 'æµé‡', 'ä»·æ ¼', 'é¢„ç®—', 'å…³é”®è¯', 'ä¼˜åŠ¿'}
        filtered_tokens = []

        for token in tokens:
            token = token.strip()
            if len(token) > 1 or token in meaningful_chars:
                if token not in self.stop_words and token not in filtered_tokens:
                    filtered_tokens.append(token)

        return filtered_tokens

    def _tokenize_filtered(self, tokens: List[str]) -> List[str]:
        """è¿›ä¸€æ­¥è¿‡æ»¤æ— æ„ä¹‰çš„è¯"""
        filtered_tokens = []
        for token in tokens:
            token = token.strip()
            if len(token) > 1 and token not in self.unmeaningful_chars:
                filtered_tokens.append(token)
        return filtered_tokens

    def _semantic_boost(self, query_tokens: List[str], doc_tokens: List[str]) -> float:
        """åŸºäºè¯­ä¹‰å…³ç³»è®¡ç®—é¢å¤–åˆ†æ•°"""
        boost = 0
        # for q_token in query_tokens:
        #     for d_token in doc_tokens:
        #         if (q_token, d_token) in self.semantic_relations:
        #             boost += self.semantic_relations[(q_token, d_token)]
        #         elif (d_token, q_token) in self.semantic_relations:
        #             boost += self.semantic_relations[(d_token, q_token)]
        #è¯­ä¹‰å¢å¼ºç›®å‰å¼ƒç”¨ï¼Œæ‰€ä»¥æ— è®ºå¦‚ä½•å‡è¿”å›0
        return boost

    def _exact_match_score(self, q_tokens: List[str], d_tokens: List[str]) -> int:
        """è®¡ç®—å®Œå…¨åŒ¹é…åˆ†æ•°"""
        matches = sum(1 for token in q_tokens if token in d_tokens)
        return matches if q_tokens else 0

    def _qa_pattern_score(self, query: str, doc_tokens: List[str]) -> float:
        """é—®ç­”æ¨¡å¼è¯†åˆ«åˆ†æ•°"""
        score = 0
        query_filtered = self._tokenize_filtered(self._tokenize(query))

        if any(word in query for word in ['ä»€ä¹ˆæ—¶å€™', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'ï¼Ÿ', 'å‘¢']):
            for query_token in query_filtered:
                if query_token in doc_tokens:
                    score += 1
        return score

    def search(self, query: str, top_k: int = 5, show_details: bool = True) -> List[Tuple[str, float, int]]:
        """
        æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹

        Returns:
            List[Tuple[æ–‡ä»¶å, æœ€ç»ˆå¾—åˆ†, æ–‡æ¡£ç´¢å¼•]]
        """
        if not self.documents:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£")

        # åˆ†è¯æŸ¥è¯¢
        tokenized_query = self._tokenize(query)

        bm25_scores_content = self.bm25_content.get_scores(tokenized_query)
        bm25_scores_title = self.bm25_title.get_scores(tokenized_query)

        final_scores = []

        for i, filename in enumerate(self.file_names):
            # BM25åˆ†æ•°
            bm25_content = bm25_scores_content[i]
            bm25_title = bm25_scores_title[i]

            # å®Œå…¨åŒ¹é…åˆ†æ•°
            exact_content = self._exact_match_score(tokenized_query, self.tokenized_documents[i])
            exact_title = self._exact_match_score(tokenized_query, self.tokenized_titles[i])

            # é—®ç­”æ¨¡å¼åˆ†æ•°
            qa_content = self._qa_pattern_score(query, self.tokenized_documents[i])
            qa_title = self._qa_pattern_score(query, self.tokenized_titles[i])

            # è¯­ä¹‰å¢å¼ºåˆ†æ•°
            semantic_content = self._semantic_boost(tokenized_query, self.tokenized_documents[i])
            semantic_title = self._semantic_boost(tokenized_query, self.tokenized_titles[i])

            # åŠ æƒèåˆ (æ ‡é¢˜æƒé‡æ›´é«˜)
            final_score_content = bm25_content + exact_content + qa_content + semantic_content * 0.1
            final_score_title = (bm25_title * 15 + exact_title * 2 + qa_title * 10 + semantic_title * 0.1)

            # æœ€ç»ˆå¾—åˆ†ï¼ˆæ ‡é¢˜æƒé‡80%ï¼Œå†…å®¹æƒé‡20%ï¼‰
            final_score = final_score_title * 0.8 + final_score_content * 0.2
            final_scores.append(final_score)

            if show_details:
                pass
                # print(f"[{filename}]:")
                # print(f"  BM25: å†…å®¹={bm25_content:.3f}, æ ‡é¢˜={bm25_title:.3f}")
                # print(f"  å®Œå…¨åŒ¹é…: å†…å®¹={exact_content}, æ ‡é¢˜={exact_title}")
                # print(f"  é—®ç­”æ¨¡å¼: å†…å®¹={qa_content:.3f}, æ ‡é¢˜={qa_title:.3f}")
                # print(f"  è¯­ä¹‰å¢å¼º: å†…å®¹={semantic_content:.3f}, æ ‡é¢˜={semantic_title:.3f}")
                # print(f"  æœ€ç»ˆå¾—åˆ†: {final_score:.6f}")
                # print()

        # æ’åºå¹¶è¿”å›ç»“æœ
        scored_docs = [(final_scores[i], self.file_names[i], i) for i in range(len(self.file_names))]
        scored_docs.sort(key=lambda x: x[0], reverse=True)

        result = [(filename, score, index) for score, filename, index in scored_docs[:top_k]]

        if show_details:
            print("")
            print(f"\nğŸ¯ å‰{top_k}ä¸ªæœ€ç›¸å…³æ–‡æ¡£:")
            for rank, (filename, score, index) in enumerate(result, 1):
                print(f"{rank}. [{filename}] - å¾—åˆ†: {score:.6f}")
                if rank == 1 and self.documents[index]:
                    # æ˜¾ç¤ºæœ€ç›¸å…³æ–‡æ¡£çš„é¢„è§ˆ
                    doc_preview = self.documents[index][:200].replace('\n', ' ').strip()
                    print(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {doc_preview}...")

        return result

    def get_best_match(self, query: str, show_details: bool = True) -> Optional[Tuple[str, str, float]]:
        """
        è·å–æœ€ä½³åŒ¹é…çš„æ–‡æ¡£

        Returns:
            Tuple[æ–‡ä»¶å, æ–‡æ¡£å†…å®¹, å¾—åˆ†] æˆ– None
        """
        results = self.search(query, top_k=1, show_details=show_details)
        if results:
            filename, score, index = results[0]
            return filename, self.documents[index], score
        return None

    def add_semantic_relation(self, word1: str, word2: str, weight: float):
        """æ·»åŠ æ–°çš„è¯­ä¹‰å…³ç³»"""
        self.semantic_relations[(word1, word2)] = weight

    def update_knowledge_base(self, new_path: Optional[str] = None):
        """æ›´æ–°çŸ¥è¯†åº“"""
        if new_path:
            self.knowledge_base_path = new_path

        # é‡æ–°åŠ è½½æ–‡æ¡£
        self.documents = []
        self.file_names = []
        self._load_documents()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = DocumentRetriever("..\config\strategies")
    # ç¤ºä¾‹æŸ¥è¯¢
    query = """åœ¨äºšé©¬é€Šå¹¿å‘Šè¿è¥ä¸­ï¼Œæ›å…‰é‡çš„è°ƒæ§éœ€è¦æ ¹æ®å¹¿å‘Šé˜¶æ®µå’Œç›®æ ‡åŠ¨æ€è°ƒæ•´ç­–ç•¥ç»„åˆã€‚å¯¹äºåˆæœŸæ¢ç´¢é˜¶æ®µï¼ŒDeepBIçš„ææ›å…‰ç­–ç•¥é€šè¿‡æ™ºèƒ½ç«ä»·è°ƒæ•´å¿«é€Ÿè·å–æ›å…‰ï¼Œé…åˆè‡ªåŠ¨åŠ è¯ç­–ç•¥å’Œè‡ªåŠ¨åŠ ASINç­–ç•¥å½¢æˆå…³é”®è¯å’Œç«å“ASINçš„æ‹“å±•é—­ç¯ï¼Œè¿™ç§åŒè½¨æœºåˆ¶æ—¢èƒ½æŒ–æ˜é«˜æ½œåŠ›é•¿å°¾è¯ï¼Œåˆèƒ½é€šè¿‡ç«å“ASINç²¾å‡†æˆªæµã€‚å½“å¹¿å‘Šè¿›å…¥ç¨³å®šæœŸåï¼Œæˆå•å…³é”®è¯ä¸ASINç­–ç•¥ä¼šå¯¹å†å²è½¬åŒ–è¯è¿›è¡Œæ¿€è¿›æä»·ï¼Œè€Œé‡ç‚¹è¯ç­–ç•¥åˆ™å¯¹è¿‘æœŸé«˜è½¬åŒ–è¯å®æ–½æ›´å¤§åŠ›åº¦çš„æº¢ä»·åŸ¹å…»ï¼Œå½¢æˆé˜¶æ¢¯å¼çš„ä»·å€¼æŒ–æ˜ä½“ç³»ã€‚é’ˆå¯¹è¿‡åº¦æ›å…‰å¯¼è‡´çš„æˆæœ¬é—®é¢˜ï¼Œæ§æ›å…‰ç­–ç•¥é€šè¿‡åŠ¨æ€é™ä»·å°†æ›å…‰æ‹‰å›åˆç†åŒºé—´ï¼Œè€Œæ§ACOSç­–ç•¥åˆ™ä¸“é—¨å‹åˆ¶é«˜èŠ±è´¹ä½è½¬åŒ–çš„åŠ£è´¨æµé‡ï¼ŒäºŒè€…ååŒå®ç°æˆæœ¬ç²¾ç»†ç®¡æ§ã€‚é¢„ç®—ç®¡ç†æ–¹é¢ï¼Œä¿®æ”¹é¢„ç®—ç­–ç•¥æ ¹æ®ACOSè¡¨ç°åŠ¨æ€è°ƒé…é¢„ç®—èµ„æºï¼Œç»“åˆåŸºäºåº“å­˜çš„é¢„ç®—è°ƒæ•´ç­–ç•¥é˜²æ­¢æ–­è´§é£é™©ï¼Œæ„æˆå®Œæ•´çš„é¢„ç®—é˜²å¾¡ä½“ç³»ã€‚å¯¹äºå¸‚åœºç«äº‰æ¿€çƒˆçš„å¤§è¯ï¼Œå¯é€šè¿‡è‡ªåŠ¨åŠ è¯ç­–ç•¥æŒç»­è·å–ç”¨æˆ·çœŸå®æœç´¢è¯ï¼Œé…åˆæœç´¢è¯ç«å“ASINç­–ç•¥æ‹“å±•æ¬¡çº§ç«å“åº“ï¼Œå®ç°æµé‡æ¥æºçš„å¤šå…ƒåŒ–å¸ƒå±€ã€‚æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹å½¢æˆ"æ¢ç´¢-ç­›é€‰-æ”¾å¤§-æ§åˆ¶"çš„é—­ç¯é€»è¾‘ï¼Œå„ç­–ç•¥é€šè¿‡æ•°æ®åé¦ˆæœºåˆ¶ç›¸äº’è”åŠ¨ï¼Œæ—¢ä¿è¯æµé‡è·å–æ•ˆç‡ï¼Œåˆç»´æŒå¥åº·çš„å¹¿å‘Šæˆæœ¬ç»“æ„ã€‚
"""
    # æœç´¢ç›¸å…³æ–‡æ¡£
    results = retriever.search(query, top_k=46)
    # è·å–æœ€ä½³åŒ¹é…

    best_match = retriever.get_best_match(query, show_details=False)
    if best_match:
        filename, content, score = best_match
        print(f"\næœ€ä½³åŒ¹é…: {filename} (å¾—åˆ†: {score:.6f})")

    # æ·»åŠ è‡ªå®šä¹‰è¯­ä¹‰å…³ç³»
    retriever.add_semantic_relation("ç­–ç•¥", "ä¼˜åŒ–", 1.5)